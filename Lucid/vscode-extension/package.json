{
  "name": "lucid-vsx",
  "publisher": "mASUm0813",
  "displayName": "Lucid (Ollama Integration)",
  "description": "Integrate local Ollama models into VS Code like as Copilot Chat",
  "version": "0.4.0",
  "license": "MIT",
  "icon": "resources/lucid-icon.png",
  "keywords": [
    "ollama",
    "ai",
    "chat",
    "copilot",
    "llm",
    "local",
    "codex"
  ],
  "galleryBanner": {
    "color": "#1e1e1e",
    "theme": "dark"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/TurkNet/lucid-vsix.git"
  },
  "engines": {
    "vscode": "^1.90.0"
  },
  "main": "./out/vscode-extension/src/extension.js",
  "categories": [
    "AI",
    "Chat"
  ],
  "activationEvents": [
    "onStartupFinished",
    "onView:lucid.chatView",
    "onCommand:lucid.sendFile",
    "onCommand:lucid.sendActiveFile",
    "onCommand:lucid.sendActiveForEdit",
    "onCommand:lucid.sendFiles",
    "onCommand:lucid.openChatView",
    "onCommand:lucid.closeChatView",
    "onCommand:lucid.dumpState"
  ],
  "contributes": {
    "chatParticipants": [
      {
        "id": "lucid.ollama",
        "name": "lucid",
        "fullName": "Lucid (Ollama)",
        "description": "Chat with your local Ollama models",
        "isSticky": true
      }
    ],
    "commands": [
      {
        "command": "lucid.setModel",
        "title": "Lucid: Set Ollama Model"
      },
      {
        "command": "lucid.sendFile",
        "title": "Lucid: Send File to Ollama"
      },
      {
        "command": "lucid.sendActiveFile",
        "title": "Lucid: Send Active File to Ollama"
      },
      {
        "command": "lucid.sendActiveForEdit",
        "title": "Lucid: Send Active Editor (selection or file) to Ollama and apply response"
      },
      {
        "command": "lucid.sendFiles",
        "title": "Lucid: Send Multiple Files to Ollama"
      },
      {
        "command": "lucid.openChatView",
        "title": "Lucid: Open Chat View"
      },
      {
        "command": "lucid.closeChatView",
        "title": "Lucid: Close Chat View"
      },
      {
        "command": "lucid.dumpState",
        "title": "Lucid: Dump Provider State"
      }
    ],
    "viewsContainers": {
      "activitybar": [
        {
          "id": "lucid",
          "title": "Lucid",
          "icon": "resources/lucid-icon.png"
        }
      ]
    },
    "views": {
      "lucid": [
        {
          "id": "lucid.chatView",
          "name": "Lucid Chat",
          "type": "webview"
        }
      ]
    },
    "configuration": {
      "title": "Lucid MCP",
      "properties": {
        "lucid.ollamaEndpoint": {
          "type": "string",
          "default": "http://localhost:11434",
          "description": "URL of the Ollama API"
        },
        "lucid.modelName": {
          "type": "string",
          "default": "llama3",
          "description": "Name of the model to use in Ollama"
        },
        "lucid.enableInlineCompletion": {
          "type": "boolean",
          "default": true,
          "description": "Enable inline code completions (ghost text) in the editor"
        },
        "lucid.inlineCompletionTemperature": {
          "type": "number",
          "default": 0.2,
          "minimum": 0,
          "maximum": 2,
          "description": "Sampling temperature used for inline completions (lower numbers are more deterministic)"
        },
        "lucid.inlineCompletionDebounceMs": {
          "type": "number",
          "default": 350,
          "minimum": 0,
          "maximum": 2000,
          "description": "Minimum delay in milliseconds between inline completion requests"
        },
        "lucid.inlineCompletionMaxRemoteChars": {
          "type": "number",
          "default": 3500,
          "minimum": 500,
          "maximum": 10000,
          "description": "Maximum combined prefix/suffix characters sent to Ollama for inline completions before falling back locally"
        },
        "lucid.ollamaExtraHeaders": {
          "type": "object",
          "default": {},
          "description": "Optional additional headers to send to Ollama API (also can set OLLAMA_EXTRA_HEADERS env var as JSON)"
        },
        "lucid.ollamaApiKey": {
          "type": "string",
          "default": "",
          "description": "Optional API key for Ollama. Can also be provided via OLLAMA_API_KEY environment variable"
        },
        "lucid.ollamaApiKeyHeaderName": {
          "type": "string",
          "default": "Authorization",
          "description": "The header name to use for the Ollama API key"
        },
        "lucid.logUnmaskedHeaders": {
          "type": "boolean",
          "default": false,
          "description": "If true, log full (unmasked) sensitive headers in curl logs"
        },
        "lucid.logUnmaskedHeadersInDev": {
          "type": "boolean",
          "default": true,
          "description": "If true, allow unmasked header logging when the extension runs in development mode"
        },
        "lucid.enableStreamingStatus": {
          "type": "boolean",
          "default": false,
          "description": "Control whether the chat view shows streaming status (spinner/disabled send) while talking to Ollama"
        }
      }
    }
  },
  "scripts": {
    "vscode:prepublish": "npm run compile",
    "compile": "tsc -p ./",
    "watch": "tsc -watch -p ./"
  },
  "devDependencies": {
    "@types/node": "^20.0.0",
    "@types/vscode": "^1.90.0",
    "typescript": "^5.0.0"
  }
}
